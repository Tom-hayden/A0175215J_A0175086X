\section{Literature Review on Artificial Neural Networks}\label{sec:litGeneral}
     \pagestyle{mario}
     \sectionauthor{M. Gini}

This section gives a literature review on the broad topic of ANN. A more specific review on ANN designed to classify the CIFAR-10 dataset is found in Section \ref{sec:litReviewCifar}. The significance and applications of ANN will be reviewed in Section \ref{subsec:signif} while recent trends and accomplishments are discussed in Section \ref{subsec:trends}.

\subsection{Significance and Applications of Artificial Neural Networks}\label{subsec:signif}

This subsection will illustrate the significance and applications of ANN. Increasing computer power shifted the focus of research towards deep ANN and similar architectures which are coined under the term "deep learning". These powerful  deep ANN are nowadays used in a variety of applications\cite{deng2014deep}\cite{lecun2015deep}.

ANN are significant because they can work as a black box model. The performance can be improved by data preprocessing, augmentation and by finding an appropriate network architecture and training process. No a-priori knowledge of the classification process itself is required. This makes deep ANN suited for applications where such knowledge is difficult to obtain. Character and speech recognition are such difficult problems, as well as image classification. In speech recognition, deep ANN have been shown to outperform other methods on a variety of speech recognition benchmarks, sometimes by a large margin\cite{hinton2012deep}. In the field of image classification, the 2012 ILSVRC (ImageNet Large-Scale Visual Recognition Challenge) marks an important turning point. A convolutional neural network (CNN) architecture won the competition for the first time - by a large margin\cite{krizhevsky}. In both fields, ANN are now widely accepted as the most powerful approach.

However, the fact that ANN do not incorporate much a-priori knowledge can also backfire. In consequence, a trained model gives little insight into its inner workings and optimal network architectures are basically found through a trial-and-error process. Most design guidelines for deep learning methods are therefore rather based on empirical knowledge than on theoretical foundations.

New methods are developed to better understand the computations deep ANN perform at each layer. The resulting visualizations reveal the process of extracting high level features out of raw input data\cite{mordvintsev2015}\cite{yosinski2015understanding}. In general, each layer extracts higher level features of the input the previous layer provides such that the features are highly abstract after a few layers. The last layer then classifies the input into one of the output categories.

\subsection{Recent Trends and Accomplishments}\label{subsec:trends}
\label{sec:Recent_trends}

Recent trends and accomplishments of ANN are described in this section. Two recent accomplishments are looked at in detail: The AlphaGo computer program and adversial examples. AlphaGo is a great example to illustrate the great capabilities of ANN. Adversial examples can easily fool very different kinds of neural networks which is a good way to exemplify the limitations the present ANN still possess.

The game Go is a complex board game with the impressive number of around $10^{170}$ legal positions\cite{tromp2006combinatorics}. Due to its enormous search space and difficulty to evaluate board positions, it is viewed as the most challenging of the classical games for artificial intelligence. A victory of a computer program over a professional human player has been considered to be at least a decade away. However, the computer program AlphaGo beat the European Go champion 5-0 in 2015\cite{silver2016mastering}.

AlphaGo makes extensive use of ANN. It consists of a "value" and a "policy" network to separately evaluate the board position and select moves. It is trained in a combination of supervised learning from human expert games and reinforcement learning through self-play. The training of such big networks requires notable computation resources. In a recent trend, dedicated hardware to train deep ANN is developed. Besides other adaptions, it is designed to speed up matrix multiplications which are one of the main components of the training process. The most notable example is the Tensor Processing Unit which achieves a 15- to 30-fold performance compared to a contemporary GPU or CPU\cite{jouppi2017datacenter}. It is important to note that the development of deep learning is closely connected to the ever improving available computing power\cite{chen2016evolution}.

AlphaGo received considerable media coverage and is considered as one of the most impressive feats of deep learning. In a follow-up paper, a further improved version of AlphaGo is presented, AlphaZero\cite{silver2017mastering}. It uses a single neural network and trains solely through reinforcement learning with self-play, starting with random play. It is only provided with the rules of Go. After only days of training, it defeated all previous versions of AlphaGo and achieved a never seen before playing strength. It is quite intriguing that even for such a complex task, the network can achieve superhuman performance without any provided knowledge besides the rules of the game.

As a second recent trend, adversarial examples recently surprised a lot of researches and became a hot topic of interest. To generate an adversarial example, a slight perturbation is applied to a correctly classified image. The classification process is then repeated and the perturbation is adapted such that the prediction error is \textit{maximized}. A slight perturbation which is not recognizable by a human is already enough to let the neural network misclassify an image with a high confidence level\cite{Nguyen_2015_CVPR}. It has been shown that adversarial examples trained on one model are likely to be misclassified by another model as well, i.e. they possess a transferability property\cite{szegedy2013intriguing}.

It is very likely that a randomly selected input to a neural network built from linear parts is processed incorrectly and the models only behave reasonably on a very thin manifold encompassing the training data\cite{goodfellow2014explaining}. This result questions the generalization abilities of ANN. Furthermore, the transferability property allows potential attacks on systems using ANN\cite{kurakin2016adversarial}\cite{papernot2017practical}. For example, stop signs could be slightly modified with stickers such that they are misclassified by autonomous vehicles which then behave unexpectedly. Further research is required to develop defense strategies against such attacks. Only then, ANN can deployed in safety critical applications.
