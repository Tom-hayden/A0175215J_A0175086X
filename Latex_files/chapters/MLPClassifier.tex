\section{Multi-Layer Perceptron Classifier}
	    \pagestyle{mario}
	    \sectionauthor{M. Gini \& T. M. Hayden} %\printinunitsof{cm}\prntlen{\linewidth} this shows linewidth in cm

This section presents the multi-layer perceptron (MLP) classifier designed to classify the CIFAR-10 dataset. It is organized as follows: Section \ref{subsec:setup} introduces the software setup used to implement the MLP classifier. Section \ref{subsec:preProp} discusses the data preprocessing and augmentation. Section \ref{subsec:netStruct} analyzes the effect of different network structures on performance. Section \ref{subsec:optNet} analyzes the effect of different hyperparameters.

\subsection{Software Setup}\label{subsec:setup}

MATLAB's Neural Networks toolbox is employed to implement the MLP classifier. The toolbox provides convenient algorithms and applications to design the MLP. A network training function with a convenient graphical user interface (GUI) to observe the progress is included as well. Figure \ref{fig:NNtool} shows the GUI.

Since this is a classification problem, parts of the network structure are given. The output of the MLP should be a prediction of to which class the input belongs to. This is accomplished with the help of the softmax function, also called normalized exponential function. Equation \ref{eq:softmax} shows the formula of such a function. A softmax layer is then used as the last layer. It gets a $K$-dimensional input vector $\boldsymbol{z}$ of arbitrary real values and "squashes" it into a $K$-dimensional output vector $\sigma(\boldsymbol{z})$ of real values in the range $[0,1]$. In our case, $K=10$ and the output values represent the probabilities that the input belongs to the respective class. The class with the highest probability then constitutes the predicition of the MLP.

\begin{equation}\label{eq:softmax}
\sigma(\boldsymbol{z})_j = \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}\quad \textrm{for}\quad j = 1,...~K.
\end{equation}

\begin{figure}[h!]
  	\centering
  	\includegraphics{images/NNtool}
  	\caption{Screenshot of MATLAB's nntraintool.}
  	\label{fig:NNtool}
\end{figure}

The other hidden layers consist of standard MLP. The input to the MLP are the pixel values of the image. Preprocessing and augmentation as described in the next section is also done before the pixel values are fed into the network.

MATLAB offers lots of adjustable settings for the MLP. Unless mentioned otherwise, the following settings are employed as default settings:

\begin{itemize}
	\item Training function: 'trainscg' which is the scaled conjugate gradient method.

	\item Loss function: 'crossentropy' which penalizes

	\item Activation function: 'tansig' which is the hyperbolic tangent sigmoid activation function

	\item The training batch size is set to 20000 images. This constitutes a compromise between a good performance and a reasonable training time.

	\item As a default network structure, two hidden layers are employed with 100 and 50 neurons.

	\item Dataset structure: 80\% are used for the training and 20\% are used for validation. For the testing, the dedicated testing batch provided in the dataset is utilized.

	\item Weight initialization: The weights of the neurons are randmoly initialized. Since this leads to slightly different performances for each run, for the performance analysis each configuration is run five times and then the test accuracy is averaged.
\end{itemize}

\FloatBarrier
\subsection{Data Preprocessing and Augmentation}\label{subsec:preProp}

Data preprocessing and augmentation take place before the data is fed into the network. In the preprocessing step, the data is normalized and centered around the mean. In the augmentation step, the amount of data is augmented through operations like image flipping.

\subsubsection{Data Preprocessing}\label{subsub:dataPreProp}

  	Each image of the dataset is represented by a 32*32*3 array, which results in 1024 pixel values per color channel. To be processed by the MLP, it is transformed into a 1*3072 array. The pixel values are integers in the range [0,255]. For normalization, the data is divided by 255 to lie within the range [0,1]. Accordingly, the datatype changes from the integer type to double. In a second step, the mean per pixel over the whole training set is subtracted. This centers the data per channel.

  	Data preprocessing also includes the division of the complete dataset into appropriate training, validation and test data batches. There are 50000 images available for training. With the default splitting into training and validation dataset (80\% for training and 20\% for validation), Figure \ref{fig:dataPreprocessing} shows the effect of increasing the training batch size as well as employing data preprocessing.

  	\begin{figure}[h!]
  		\centering
   		\includegraphics{images/dataPreprocessing}
   		\caption{Comparison of network performance with and without data preprocessing. The errorbars represent one standard deviation.}
   		\label{fig:dataPreprocessing}
   	\end{figure}

   	As intuitively expected, the test accuracy increases for an increasing training batch size. However, the data preprocessing does not lead to a significant increase of performance. The normalization and mean subtraction do not have a big influence in that specific case since the input values already lie in a well defined range and the mean per pixel is also a almost constant value over all pixels. The errorbars show that the different accuracies are most likely due to the random weight initialization.

\FloatBarrier
\subsubsection{Data Augmentation}

   	Figure \ref{fig:dataPreprocessing} above shows that a larger training batch size leads to an increased performance. A natural approach is therefore to artificially increase the training batch size. Image flipping and image rotation are used. Figure \ref{fig:dataAugmentation} illustrates the performance gain from vertical image mirroring. The test accuracy is significantly increased for all training batch sizes by around \SI{3}{\percent}.

   	\begin{figure}[h!]
		\centering
   	  	\includegraphics{images/dataAugmentation}
   	  	\caption{Comparison of network performance with and without data augmentation. The errorbars represent one standard deviation.}
   	  	\label{fig:dataAugmentation}
   	\end{figure}
\FloatBarrier

\subsection{Optimization of Network Structure}\label{subsec:netStruct}

Choosing the correct architecture of a neural network remains a complicated area of study which is still not fully understood\cite{andersen1999cross}. For any given problem there are essentially an infinite number of valid MLP architectures. There are many different approaches to architecture selection but none are foolproof\cite{andersen1999cross}.

\subsubsection{Layers of the MLP}

Every neural network, including MLPs, will have at the very least one input layer and one output layer. The size of the input layer simply depends on the size of the input data. In the case of CIFAR-10 the input size is $32\times32\times3$, the size of the input image data.

In addition to the input layer, each network will also have an output layer. The size of the output layer is also defined by the format of the data. In the the case of CIFAR-10, the output layer is a softmax layer with 10 nodes corresponding to each class label.

There can be an arbitrarily large number of hidden layers. However performance gains from addition additional layers beyond the first are very small or even negligible. Additionally adding more layers increases the chance that the classifier finds a local minima\cite{de1993backpropagation}. Figure \ref{fig:layers} shows the effect of adding additional layers to our MLP classifier. Note that adding additional layers after the 2nd has almost no impact on the test accuracy.

\begin{figure}[h!]
    \centering
    \includegraphics{images/numberlayers}
    \caption{Effect of adding additional layers to the MLP classifier}
    \label{fig:layers}
 \end{figure}


c)on the training of the MLP
\begin{itemize}
   	\item Varying the number of neurons

	\begin{figure}[h!]
   		 \centering
   		 \includegraphics{images/surfacelayers}
   		 \caption{Hello Boy2}
   		 \label{fig:surfaceLayers}
    \end{figure}

	\item Varying the number of layers

	  	\begin{figure}[h!]
	  		\centering
	  		\includegraphics{images/numberlayers}
	  		\caption{Hello Boy2}
	  		\label{fig:test2}
	  	\end{figure}
\end{itemize}

\FloatBarrier
\subsection{Optimization of Network Hyperparameters}\label{subsec:optNet}
Hyperparameter selection for neural networks such as MLPs has become an interesting field of research with many interesting algorithms being used to estimate good parameters to use\cite{bergstra2011algorithms}. However, since these algorithms rely on performing many trials and updating the parameters accordingly, they proved unsuitable for our purposes due to our limited resources. Our approach instead relied on performing a limited number of trails to find trends. From these general trends and our knowledge of MLP classifiers, we then estimated good parameters to use.


d) on the performance of the MLP with different objective functions and optimization methods

\textcolor{red}{Some pic of the confusion matrix}

\begin{itemize}
   	\item Different learning rates

   	\item Different optimization methods

   	\item Different performance functions
   	There are six different performance functions available in the MATLAB environment:


\end{itemize}
e)any other interesting observation that you think are pertinent (e.g. effect of learning rate on convergence speed).

\subsubsection{Learning Rate}

Choosing the learning rate for a MLP classifier can be a challenging process. There is no approach that will work optimally for every dataset. A learning rate that is too high can overshoot the solution and become unstable. However low learning rates can become stuck in local minima or take a long time to train. A good solution is to pick a high learning rate that can pass over local minima and  to gradually decrease the learning rate so that the classifier does not become unstable. This will result in a classifier which initially follows general trends and 'explores' a large portion of the parameter space. Later on the smaller learning rate will allow for the model to be fine tuned into a particular solution.

The effects of changing the learning rate in our model can be seen in figure \ref{fig:learningRate}. The figure shows that increased learning rates, in general, had a positive effect on model accuracy.


\begin{figure}[h!]
    \centering
    \includegraphics{images/learningRate.eps}
    \caption{2+2=}
    \label{fig:learningRate}
 \end{figure}

 \subsubsection{Performance function}

 \subsubsection{Activation function}

 \subsubsection{Training fucntion}
