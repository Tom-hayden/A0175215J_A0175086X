\section{Multi-Layer Perceptron Classifier}
	    \pagestyle{mario}
	    \sectionauthor{M. Gini \& T. M. Hayden} %\printinunitsof{cm}\prntlen{\linewidth} this shows linewidth in cm

This section presents the multi-layer perceptron (MLP) classifier designed to classify the CIFAR-10 dataset. It is organized as follows: Section \ref{subsec:setup} introduces the software setup used to implement the MLP classifier. Section \ref{subsec:preProp} discusses the data preprocessing and augmentation. Section \ref{subsec:netStruct} analyzes the effect of different network structures on performance. Section \ref{subsec:optNet} analyzes the effect of different hyperparameters.

\subsection{Software Setup}\label{subsec:setup}

MATLAB's Neural Networks toolbox is employed to implement the MLP classifier. The toolbox provides convenient algorithms and applications to design the MLP. A network training function with a convenient graphical user interface (GUI) to observe the progress is included as well. Figure \ref{fig:NNtool} shows the GUI.

Since this is a classification problem, parts of the network structure are given. The output of the MLP should be a prediction of to which class the input belongs to. This is accomplished with the help of the softmax function, also called normalized exponential function. Equation \ref{eq:softmax} shows the formula of such a function. A softmax layer is then used as the last layer. It gets a $K$-dimensional input vector $\boldsymbol{z}$ of arbitrary real values and "squashes" it into a $K$-dimensional output vector $\sigma(\boldsymbol{z})$ of real values in the range $[0,1]$. In our case, $K=10$ and the output values represent the probabilities that the input belongs to the respective class. The class with the highest probability then constitutes the predicition of the MLP.

\begin{equation}\label{eq:softmax}
\sigma(\boldsymbol{z})_j = \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}\quad \textrm{for}\quad j = 1,...~K.
\end{equation}

\begin{figure}[h!]
  	\centering
  	\includegraphics{images/NNtool}
  	\caption{Screenshot of MATLAB's nntraintool.}
  	\label{fig:NNtool}
\end{figure}

The other hidden layers consist of standard MLP. The input to the MLP are the pixel values of the image. Preprocessing and augmentation as described in the next section is also done before the pixel values are fed into the network.

MATLAB offers lots of adjustable settings for the MLP. Unless mentioned otherwise, the following settings are employed as default settings:

\begin{itemize}
	\item Training function: 'trainscg' which is the scaled conjugate gradient method.
	
	\item Loss function: 'crossentropy' which penalizes
	
	\item Activation function: 'tansig' which is the hyperbolic tangent sigmoid activation function 
	
	\item The training batch size is set to 20000 images. This constitutes a compromise between a good performance and a reasonable training time.
	
	\item As a default network structure, two hidden layers are employed with 100 and 50 neurons.
	
	\item Dataset structure: 80\% are used for the training and 20\% are used for validation. For the testing, the dedicated testing batch provided in the dataset is utilized.
	
	\item Weight initialization: The weights of the neurons are randmoly initialized. Since this leads to slightly different performances for each run, for the performance analysis each configuration is run five times and then the test accuracy is averaged.
\end{itemize}

\FloatBarrier
\subsection{Data Preprocessing and Augmentation}\label{subsec:preProp}

Data preprocessing and augmentation take place before the data is fed into the network. In the preprocessing step, the data is normalized and centered around the mean. In the augmentation step, the amount of data is augmented through operations like image flipping. 
 
\subsubsection{Data Preprocessing}\label{subsub:dataPreProp}

  	Each image of the dataset is represented by a 32*32*3 array, which results in 1024 pixel values per color channel. To be processed by the MLP, it is transformed into a 1*3072 array. The pixel values are integers in the range [0,255]. For normalization, the data is divided by 255 to lie within the range [0,1]. Accordingly, the datatype changes from the integer type to double. In a second step, the mean per pixel over the whole training set is subtracted. This centers the data per channel.
  	
  	Data preprocessing also includes the division of the complete dataset into appropriate training, validation and test data batches. There are 50000 images available for training. With the default splitting into training and validation dataset (80\% for training and 20\% for validation), Figure \ref{fig:dataPreprocessing} shows the effect of increasing the training batch size as well as employing data preprocessing.
  	
  	\begin{figure}[h!]
  		\centering
   		\includegraphics{images/dataPreprocessing}
   		\caption{Comparison of network performance with and without data preprocessing. The errorbars represent one standard deviation.}
   		\label{fig:dataPreprocessing}
   	\end{figure}
   	
   	As intuitively expected, the test accuracy increases for an increasing training batch size. However, the data preprocessing does not lead to a significant increase of performance. The normalization and mean subtraction do not have a big influence in that specific case since the input values already lie in a well defined range and the mean per pixel is also a almost constant value over all pixels. The errorbars show that the different accuracies are most likely due to the random weight initialization.
   	
\FloatBarrier
\subsubsection{Data Augmentation}
	    	
   	Figure \ref{fig:dataPreprocessing} above shows that a larger training batch size leads to an increased performance. A natural approach is therefore to artificially increase the training batch size. Image flipping and image rotation are used. Figure \ref{fig:dataAugmentation} illustrates the performance gain from vertical image mirroring. The test accuracy is significantly increased for all training batch sizes by around \SI{3}{\percent}.
   	
   	\begin{figure}[h!]
		\centering
   	  	\includegraphics{images/dataAugmentation}
   	  	\caption{Comparison of network performance with and without data augmentation. The errorbars represent one standard deviation.}
   	  	\label{fig:dataAugmentation}
   	\end{figure}
\FloatBarrier

\subsection{Optimization of Network Structure}\label{subsec:netStruct}
c)on the training of the MLP    
\begin{itemize}
   	\item Varying the number of neurons	
   	
	\begin{figure}[h!]
   		 \centering	  		
   		 \includegraphics{images/surfacelayers}
   		 \caption{Hello Boy2}
   		 \label{fig:surfaceLayers}
    \end{figure}
   	
	\item Varying the number of layers
	
	  	\begin{figure}[h!]
	  		\centering	  		
	  		\includegraphics{images/numberlayers}
	  		\caption{Hello Boy2}
	  		\label{fig:test2}
	  	\end{figure}	
\end{itemize}

\FloatBarrier
\subsection{Optimization of Network Hyperparameters}\label{subsec:optNet}
d) on the performance of the MLP with different objective functions and optimization methods

\textcolor{red}{Some pic of the confusion matrix}

\begin{itemize}
   	\item Different learning rates
	    	
   	\item Different optimization methods
   	
   	\item Different performance functions
   	There are six different performance functions available in the MATLAB environment:
   	
   
\end{itemize}
e)any other interesting observation that you think are pertinent (e.g. effect of learning rate on convergence speed).
	  	\begin{figure}[h!]
	  		\centering	  		
	  		\includegraphics{images/activationFct}
	  		\caption{Hello Boy2}
	  		\label{fig:test2}
	  	\end{figure}
	  	
	  	\begin{figure}[h!]
	  		\centering	  		
	  		\includegraphics{images/performFct}
	  		\caption{Hello Boy2}
	  		\label{fig:test2}
	  	\end{figure}	  	


